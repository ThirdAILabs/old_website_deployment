<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on ThirdAI</title>
    <link>//thirdailabs.github.io/tags/blog/</link>
    <description>Recent content in Blog on ThirdAI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="//thirdailabs.github.io/tags/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training Bottlenecks: A Harsh Reality of AI Models</title>
      <link>//thirdailabs.github.io/resources/blog/training-bottlenecks/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>//thirdailabs.github.io/resources/blog/training-bottlenecks/</guid>
      <description>There is no silver bullet AI modality. Better predictions require a constant cycle of feature engineering, hyper-parameter tuning, training, and testing the resultant models. The most time-consuming stage of any AI-powered application pipeline is fine tuning, which often requires repeated iterations of network training to expose performance gains. Even with AutoML and Neural Architecture Search (NAS), engineering features and hammering out other task-specific pipelines in an AI system is unavoidable.</description>
    </item>
    
  </channel>
</rss>
