<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT | ThirdAI</title><meta name=keywords content="BOLT,Deep Learning,Sustainability,Carbon Footprint,Training,AI"><meta name=description content="Sustainability is rarely factored into the calculus of selecting and training deep learning models&mdash;in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT&mdash;and more broadly, sparsity&mdash;translate into carbon savings?
Inspiration After reading Etsy&rsquo;s Cloud Jewels blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity."><meta name=author content="Benjamin Meisburger"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=//thirdailabs.github.io/static/css/stylesheet.min.1c94ba4c0a25eccefdf9543ec7749bcbbdd8dd8c8b768b817fdf58061191d4cb.css integrity="sha256-HJS6TAol7M79+VQ+x3Sby73Y3YyLdouBf99YBhGR1Ms=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=//thirdailabs.github.io/static/js/highlight.min.5aa56afcda8336fdb08c2616229a74b5747c15486b4ab36b99e15e84ccac18a1.js integrity="sha256-WqVq/NqDNv2wjCYWIpp0tXR8FUhrSrNrmeFehMysGKE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT"><meta property="og:description" content="Sustainability is rarely factored into the calculus of selecting and training deep learning models&mdash;in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT&mdash;and more broadly, sparsity&mdash;translate into carbon savings?
Inspiration After reading Etsy&rsquo;s Cloud Jewels blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity."><meta property="og:type" content="article"><meta property="og:url" content="//thirdailabs.github.io/resources/blog/carbon-footprint-of-bolt/"><meta property="article:section" content="resources"><meta property="article:published_time" content="2022-06-08T12:23:41-05:00"><meta property="article:modified_time" content="2022-06-08T12:23:41-05:00"><meta property="og:site_name" content="ThirdAI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT"><meta name=twitter:description content="Sustainability is rarely factored into the calculus of selecting and training deep learning models&mdash;in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT&mdash;and more broadly, sparsity&mdash;translate into carbon savings?
Inspiration After reading Etsy&rsquo;s Cloud Jewels blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Resources","item":"//thirdailabs.github.io/resources/"},{"@type":"ListItem","position":2,"name":"Blog","item":"//thirdailabs.github.io/resources/blog/"},{"@type":"ListItem","position":3,"name":"Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT","item":"//thirdailabs.github.io/resources/blog/carbon-footprint-of-bolt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT","name":"Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT","description":"Sustainability is rarely factored into the calculus of selecting and training deep learning models\u0026mdash;in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT\u0026mdash;and more broadly, sparsity\u0026mdash;translate into carbon savings?\nInspiration After reading Etsy\u0026rsquo;s Cloud Jewels blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity.","keywords":["BOLT","Deep Learning","Sustainability","Carbon Footprint","Training","AI"],"articleBody":"Sustainability is rarely factored into the calculus of selecting and training deep learning models—in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT—and more broadly, sparsity—translate into carbon savings?\nInspiration After reading Etsy’s Cloud Jewels blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity. My goals were twofold: first, to determine if our BOLT engine provides a meaningful reduction in carbon footprint when compared to a state-of-the-art equivalent, and second: to determine what extent sparsity impacts BOLT’s net carbon footprint.\nMethodology For my tests, I chose a straightforward task: sentiment classification on Yelp reviews. For this benchmark, there is a well-defined state-of-the-art model—RoBERTa, fine-tuned for sentiment.\nTo standardize our testing as much as possible, all experiments were run on AWS instances in zone us-west-1 which provides a replicable and consistent framework. First, we fine-tuned off-the-shelf RoBERTa on an p4d.24xlarge instance, which reached ~93% accuracy in 40 minutes restricted to a single A100 GPU (a p4d.24xlarge instance includes eight A100s). We used 93% accuracy as our threshold for all subsequent models. Next, we trained BOLT on an r6g.xlarge instance with 20% sparsity—which reached ~93% accuracy in 42 minutes. We repeated this experiment with 10% sparsity, resulting in a training time of just over 20 minutes. Finally, we trained BOLT with 5% sparsity—however, this particular model only reached 90% accuracy, so fewer comparisons can be drawn.\nI used this dataset (published under a Creative Commons Attribution 4.0 International License) which aggregates power consumptions and manufacturing footprint esimations for various EC2 instances combined with the methodology described here.\nI used the following formula to estimate the carbon footprint of a given AWS instance:\nFor example, when estimating the emissions from fine-tuning RoBERTa on a single A100 within a p4d.24xlarge instance:\nThis process was repeated for each experiment, compiled in the results below.\nResults RoBERTa was fine-tuned on a single A100 GPU within a p4d.24xlarge AWS instance. All BOLT networks were trained from scratch on a single r6g.xlarge instance. All models were trained to comparable accuracies.\nAnalysis As shown above, the BOLT engine provides a signicant improvement over state-of-the-art models in regards to carbon footprint; on average producing just 2.5% the carbon emissions of RoBERTa fine-tuned on a GPU. Within the BOLT models, it should be noted that increasing sparsity yields a significant—but not quite linear—carbon savings, although the emissions begins to plateau as the total carbon footprint approaches the (unavoidable) cost of manufacturing a CPU. It should be noted that these figures are estimates only, but we are confident they are conservative enought to capture procedural error and thus should be representative of the true footprint.\n","wordCount":"481","inLanguage":"en","datePublished":"2022-06-08T12:23:41-05:00","dateModified":"2022-06-08T12:23:41-05:00","author":{"@type":"Person","name":"Benjamin Meisburger"},"mainEntityOfPage":{"@type":"WebPage","@id":"//thirdailabs.github.io/resources/blog/carbon-footprint-of-bolt/"},"publisher":{"@type":"Organization","name":"ThirdAI","logo":{"@type":"ImageObject","url":"//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=list id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><div class=hero><header class=header><nav class=nav><div class=logo><a href=//thirdailabs.github.io/ accesskey=h title="ThirdAI (Alt + H)"><img class=icon alt=logo aria-label=logo height=35></img>ThirdAI</a>
<span class=logo-switches></span></div><ul id=menu><li><div class=dropdown><button class=drop-button>Solutions</button><div class=drop-content><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a>
<a href=//thirdailabs.github.io/solutions/text-classification>Sentiment Classification</a>
<a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></div></div></li><li><a href=//thirdailabs.github.io/engine/bolt><div class=menu-item>Engine</div></a></li><li><div class=dropdown><button class=drop-button>Resources</button><div class=drop-content><a href=//thirdailabs.github.io/resources/documentation>Documentation</a>
<a href=//thirdailabs.github.io/resources/webinars>Webinars</a>
<a href=//thirdailabs.github.io/resources/blog>Blog</a>
<a href=//thirdailabs.github.io/resources/demo>Demo</a></div></div></li><li><div class=dropdown><button class=drop-button>Company</button><div class=drop-content><a href=//thirdailabs.github.io/company/about>About</a>
<a href=//thirdailabs.github.io/company/careers/>Careers</a></div></div></li><li><a href=//thirdailabs.github.io/contact/><div class=menu-item>Contact</div></a></li></ul><input class=checkbox type=checkbox><div class=hamburger-lines><span class="line line1"></span>
<span class="line line2"></span>
<span class="line line3"></span></div><div class=mobile-menu-items><li><a href=//thirdailabs.github.io/solutions/>Solutions</a></li><li><a href=//thirdailabs.github.io/engine/bolt>Engine</a></li><li><a href=//thirdailabs.github.io/resources/>Resources</a></li><li><a href=//thirdailabs.github.io/company/>Company</a></li><li><a href=//thirdailabs.github.io/contact/>Contact</a></li></div></nav></header></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//thirdailabs.github.io/>Home</a>&nbsp;»&nbsp;<a href=//thirdailabs.github.io/resources/blog/>Blog</a>&nbsp;»&nbsp;<a>Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT</a></div><h1 class=post-title>Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT</h1><div class=post-meta><span title='2022-06-08 12:23:41 -0500 -0500'>June 8, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Benjamin Meisburger</div></header><div class=post-content><p>Sustainability is rarely factored into the calculus of selecting and training deep learning models&mdash;in the case of GPT-3, for instance, the electricity cost of training alone was reported to be $12 million. And as model sizes continue to grow, their associated environmental costs begin to burgeon. But how does BOLT&mdash;and more broadly, sparsity&mdash;translate into carbon savings?</p><h3 id=inspiration>Inspiration</h3><p>After reading <a href=https://www.etsy.com/codeascraft/cloud-jewels-estimating-kwh-in-the-cloud>Etsy&rsquo;s Cloud Jewels</a> blog post, I was curious to see if we could replicate their methodology to ballpark the carbon footprint required to train a model with BOLT at different levels of sparsity. My goals were twofold: first, to determine if our BOLT engine provides a meaningful reduction in carbon footprint when compared to a state-of-the-art equivalent, and second: to determine what extent sparsity impacts BOLT&rsquo;s net carbon footprint.</p><h3 id=methodology>Methodology</h3><p>For my tests, I chose a straightforward task: sentiment classification on Yelp reviews. For this benchmark, there is a well-defined state-of-the-art model&mdash;RoBERTa, fine-tuned for sentiment.</p><p>To standardize our testing as much as possible, all experiments were run on AWS instances in zone us-west-1 which provides a replicable and consistent framework. First, we fine-tuned off-the-shelf RoBERTa on an p4d.24xlarge instance, which reached ~93% accuracy in 40 minutes restricted to a single A100 GPU (a p4d.24xlarge instance includes eight A100s). We used 93% accuracy as our threshold for all subsequent models. Next, we trained BOLT on an r6g.xlarge instance with 20% sparsity&mdash;which reached ~93% accuracy in 42 minutes. We repeated this experiment with 10% sparsity, resulting in a training time of just over 20 minutes. Finally, we trained BOLT with 5% sparsity&mdash;however, this particular model only reached 90% accuracy, so fewer comparisons can be drawn.</p><p>I used this <a href="https://docs.google.com/spreadsheets/d/1DqYgQnEDLQVQm5acMAhLgHLD8xXCG9BIrk-_Nv6jF3k/edit#gid=504755275">dataset</a> (published under a Creative Commons Attribution 4.0 International License) which aggregates power consumptions and manufacturing footprint esimations for various EC2 instances combined with the methodology described <a href=https://medium.com/teads-engineering/building-an-aws-ec2-carbon-emissions-dataset-3f0fd76c98ac>here</a>.</p><p>I used the following formula to estimate the carbon footprint of a given AWS instance:</p><img class=carbon-equation alt="Equation for Calculating Carbon Footprint" style=margin-top:40px;margin-bottom:40px><p>For example, when estimating the emissions from fine-tuning RoBERTa on a single A100 within a p4d.24xlarge instance:</p><img class=carbon-equation2 alt="Equation for Calculating Carbon Footprint" style=margin-top:10px;margin-bottom:30px><p>This process was repeated for each experiment, compiled in the results below.</p><h3 id=results>Results</h3><img class=carbon-footprint-graph alt="Carbon Footprint vs. Training Time for Various Models" style=max-width:80%;margin-top:40px;margin-bottom:30px><p>RoBERTa was fine-tuned on a single A100 GPU within a p4d.24xlarge AWS instance. All BOLT networks were trained from scratch on a single r6g.xlarge instance. All models were trained to comparable accuracies.</p><h3 id=analysis>Analysis</h3><p>As shown above, the BOLT engine provides a signicant improvement over state-of-the-art models in regards to carbon footprint; on average producing just 2.5% the carbon emissions of RoBERTa fine-tuned on a GPU. Within the BOLT models, it should be noted that increasing sparsity yields a significant—but not quite linear—carbon savings, although the emissions begins to plateau as the total carbon footprint approaches the (unavoidable) cost of manufacturing a CPU. It should be noted that these figures are estimates only, but we are confident they are conservative enought to capture procedural error and thus should be representative of the true footprint.</p></div><footer class=post-footer><ul class=post-tags><li><a href=//thirdailabs.github.io/tags/bolt/>BOLT</a></li><li><a href=//thirdailabs.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=//thirdailabs.github.io/tags/sustainability/>Sustainability</a></li><li><a href=//thirdailabs.github.io/tags/carbon-footprint/>Carbon Footprint</a></li><li><a href=//thirdailabs.github.io/tags/training/>Training</a></li><li><a href=//thirdailabs.github.io/tags/ai/>AI</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT on twitter" href="https://twitter.com/intent/tweet/?text=Sparsity%20and%20Sustainability%3a%20Estimating%20the%20Carbon%20Footprint%20of%20BOLT&url=%2f%2fthirdailabs.github.io%2fresources%2fblog%2fcarbon-footprint-of-bolt%2f&hashtags=BOLT%2cDeepLearning%2cSustainability%2cCarbonFootprint%2cTraining%2cAI"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2f%2fthirdailabs.github.io%2fresources%2fblog%2fcarbon-footprint-of-bolt%2f&title=Sparsity%20and%20Sustainability%3a%20Estimating%20the%20Carbon%20Footprint%20of%20BOLT&summary=Sparsity%20and%20Sustainability%3a%20Estimating%20the%20Carbon%20Footprint%20of%20BOLT&source=%2f%2fthirdailabs.github.io%2fresources%2fblog%2fcarbon-footprint-of-bolt%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Sparsity and Sustainability: Estimating the Carbon Footprint of BOLT on facebook" href="https://facebook.com/sharer/sharer.php?u=%2f%2fthirdailabs.github.io%2fresources%2fblog%2fcarbon-footprint-of-bolt%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></footer></article></main><footer class=fat-footer><div class=wrapper><div class=fat-footer-social><a href=https://twitter.com/ThirdAILab><img src=//thirdailabs.github.io/images/twitter.svg width=50px class=footer-icon></a>
<a href=https://www.linkedin.com/company/thirdai-corp><img src=//thirdailabs.github.io/images/linkedin.svg width=45px class=footer-icon></a>
<a href=#><img src=//thirdailabs.github.io/images/facebook.svg width=22px class=footer-icon></a></div><div class=fat-footer-menus><div class=menu-container><div class=menu-title><h3>solutions</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a></li><li><a href=//thirdailabs.github.io/solutions/text-classification>Text Classification</a></li><li><a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>engine</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/engine/bolt>BOLT</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>resources</h3></div><div class=menu-content><ul><li><a href=#>Documentation</a></li><li><a href=#>Webinars</a></li><li><a href=//thirdailabs.github.io/resources/blog>Blog</a></li><li><a href=//thirdailabs.github.io/resources/demo>Demo</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>company</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/company/about>About us</a></li><li><a href=//thirdailabs.github.io/company/careers>Careers</a></li><li><a href=//thirdailabs.github.io/contact>Contact</a></li></ul></div></div></div></div><div class=copyright-wrapper><div class=logo><a href=//thirdailabs.github.io/><img src=//thirdailabs.github.io/images/logo-dark.png alt=logo height=35></img>ThirdAI</a></div><div class=copyright><span>&copy; 2022 <a href=//thirdailabs.github.io/>ThirdAI</a></span></div></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>