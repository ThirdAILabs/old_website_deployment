<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Product Recommendation | Third AI</title><meta name=keywords content="Case Study,Product Recommendation,BOLT"><meta name=description content="The BOLT engine can train a 1.6-billion parameter model on a <$1,200 refurbished V3 Intel machine faster than four top-of-the-line Nvidia A100 GPUs."><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=//thirdailabs.github.io/static/css/stylesheet.min.2b65407eaa69bd34c5c68586f0d2ea90d4de57878dab3703c9d50c07b038c297.css integrity="sha256-K2VAfqppvTTFxoWG8NLqkNTeV4eNqzcDydUMB7A4wpc=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=//thirdailabs.github.io/static/js/highlight.min.b801c3817c6b951b39b4f5adde05c84b6d72be498309c6b09454d361bf3f3b02.js integrity="sha256-uAHDgXxrlRs5tPWt3gXIS21yvkmDCcawlFTTYb8/OwI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.99.1"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Product Recommendation"><meta property="og:description" content="The BOLT engine can train a 1.6-billion parameter model on a <$1,200 refurbished V3 Intel machine faster than four top-of-the-line Nvidia A100 GPUs."><meta property="og:type" content="article"><meta property="og:url" content="//thirdailabs.github.io/solutions/product-recommendation/"><meta property="og:image" content="//thirdailabs.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="solutions"><meta property="og:site_name" content="ThirdAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="//thirdailabs.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Product Recommendation"><meta name=twitter:description content="The BOLT engine can train a 1.6-billion parameter model on a <$1,200 refurbished V3 Intel machine faster than four top-of-the-line Nvidia A100 GPUs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Solutions","item":"//thirdailabs.github.io/solutions/"},{"@type":"ListItem","position":2,"name":"Product Recommendation","item":"//thirdailabs.github.io/solutions/product-recommendation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Product Recommendation","name":"Product Recommendation","description":"The BOLT engine can train a 1.6-billion parameter model on a \u0026lt;$1,200 refurbished V3 Intel machine faster than four top-of-the-line Nvidia A100 GPUs.","keywords":["Case Study","Product Recommendation","BOLT"],"articleBody":"Fully connected networks are ubiquitous in recommendation systems and NLP tasks. State of the art models like Facebook’s DLRM, Microsoft’s DSSM, and Amazon’s DSSM are all giant fully-connected neural networks.\nTo benchmark BOLT, we trained two networks: one fully-connected network with a 256-dimensional hidden layer (200 million parameters) and one fully connected network with a 2000-dimensional hidden layer (1.6 billion parameters). The aforementioned industry-scale recommendation models have 200 to 500 million parameters. Even BERT-Large is in the same ballpark, with 340 million parameters. For a task representative of several industry-scale applications, we choose the Amazon 670K Kaggle dataset.\nTraining a 200M Parameter Model The table below summarizes our findings: BOLT trains the smaller, 200 million parameter network faster than Tensorflow on all tested CPUs–whether the chips are M1, AMD, or Intel.\n   Processor Average price (USD) Training time per epoch: BOLT (s) Training time per epoch: TensorFlow (s)     Apple M1 ~$1,200 814 7200   AMD EPYC 7000 ~$1,750 644 2100   Intel Xeon Platinum 8000 ~$7,500 300 2432   Nvidia A100 (reference) ~$12,000 - 1000    Typically, larger models need larger batch sizes for speed and generalization. However, GPUs have limited memory and cannot scale to the needs of a billion-parameter network. We found that a top-of-the-line A100 GPU with 48GB of memory can barely accommodate a batch size of 256 for our 1.6B network.\nOn this giant model, a batch size of 2048–which gives better accuracy than a batch size of 256–runs out of memory on an A100 GPU. Even distributing the model on two A100s is not enough; we need at least four A100 GPUs and nearly 200GB of memory to train the 1.6 billion parameter model with a batch size of 2048.\nTraining a 1.6B Parameter Model On the other hand, our BOLT engine can effortlessly scale to a large batch size with no change in model memory. We can even train with a batch size of 10,000. Notably, training this model using TensorFlow on CPUs is around 5x slower than BOLT.\n   Processor Average price (USD) Training time per epoch: BOLT (s) Training time per epoch: TensorFlow (s)     Intel Xeon E5-2680 ~$1,200 1369 5875   Nvidia A100 ~$12,000 - Insufficient memory   2x Nvidia A100 ~$24,000 - Insufficient memory   4x Nvidia A100 ~$50,000 - 1648    A billion parameter model with a batch size of 2048 requires a fleet of four A100s to train, each costing more than $10K. In contrast, our BOLT engine can train the same model faster on a 24-core refurbished V3 Intel machine that costs less than $1,200.\nEager to try it out yourself? Contact us for more details or request a demo.\n","wordCount":"432","inLanguage":"en","image":"//thirdailabs.github.io/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"//thirdailabs.github.io/solutions/product-recommendation/"},"publisher":{"@type":"Organization","name":"Third AI","logo":{"@type":"ImageObject","url":"//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=list id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><div class=hero><header class=header><nav class=nav><div class=logo><a href=//thirdailabs.github.io/ accesskey=h title="ThirdAI (Alt + H)"><img class=icon alt=logo aria-label=logo height=35></img>ThirdAI</a>
<span class=logo-switches></span></div><ul id=menu><li><div class=dropdown><button class=drop-button>Solutions</button><div class=drop-content><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a>
<a href=//thirdailabs.github.io/solutions/text-classification>Sentiment Classification</a>
<a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></div></div></li><li><a href=//thirdailabs.github.io/engine/bolt><div class=menu-item>Engine</div></a></li><li><div class=dropdown><button class=drop-button>Resources</button><div class=drop-content><a href=//thirdailabs.github.io/resources/documentation>Documentation</a>
<a href=//thirdailabs.github.io/resources/webinars>Webinars</a>
<a href=//thirdailabs.github.io/resources/blog>Blog</a>
<a href=//thirdailabs.github.io/resources/demo>Demo</a></div></div></li><li><div class=dropdown><button class=drop-button>Company</button><div class=drop-content><a href=//thirdailabs.github.io/company/about>About</a>
<a href=//thirdailabs.github.io/company/careers/>Careers</a></div></div></li><li><a href=//thirdailabs.github.io/contact/><div class=menu-item>Contact</div></a></li></ul><input class=checkbox type=checkbox><div class=hamburger-lines><span class="line line1"></span>
<span class="line line2"></span>
<span class="line line3"></span></div><div class=mobile-menu-items><li><a href=//thirdailabs.github.io/solutions/>Solutions</a></li><li><a href=//thirdailabs.github.io/engine/bolt>Engine</a></li><li><a href=//thirdailabs.github.io/resources/>Resources</a></li><li><a href=//thirdailabs.github.io/company/>Company</a></li><li><a href=//thirdailabs.github.io/contact/>Contact</a></li></div></nav></header></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//thirdailabs.github.io/>Home</a>&nbsp;»&nbsp;<a href=//thirdailabs.github.io/solutions/>Solutions</a>&nbsp;»&nbsp;<a>Product Recommendation</a></div><h1 class=post-title>A Product Recommendation Case Study: Training Fully-Connected Networks</h1><div class=post-meta></div></header><div class=post-content><img class=bolt-graph alt="BOLT Benchmarks"><p>Fully connected networks are ubiquitous in recommendation systems and NLP tasks. State of the art models like <a href=https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/>Facebook’s DLRM</a>, <a href=https://www.microsoft.com/en-us/research/project/dssm/>Microsoft’s DSSM</a>, and <a href=https://arxiv.org/abs/1907.00937>Amazon’s DSSM</a> are all giant fully-connected neural networks.</p><p>To benchmark BOLT, we trained two networks: one fully-connected network with a 256-dimensional hidden layer (200 million parameters) and one fully connected network with a 2000-dimensional hidden layer (1.6 billion parameters). The aforementioned industry-scale recommendation models have 200 to 500 million parameters. Even BERT-Large is in the same ballpark, with 340 million parameters. For a task representative of several industry-scale applications, we choose the <a href=https://www.kaggle.com/c/extreme-classification-amazon>Amazon 670K Kaggle dataset</a>.</p><h2 id=training-a-200m-parameter-model>Training a 200M Parameter Model</h2><img class=cpu-comparison alt="Training Time vs. CPU"><p>The table below summarizes our findings: BOLT trains the smaller, 200 million parameter network faster than Tensorflow on all tested CPUs&ndash;whether the chips are M1, AMD, or Intel.</p><table><thead><tr><th>Processor</th><th>Average price (USD)</th><th>Training time per epoch: BOLT (s)</th><th>Training time per epoch: TensorFlow (s)</th></tr></thead><tbody><tr><td>Apple M1</td><td>~$1,200</td><td>814</td><td>7200</td></tr><tr><td>AMD EPYC 7000</td><td>~$1,750</td><td>644</td><td>2100</td></tr><tr><td>Intel Xeon Platinum 8000</td><td>~$7,500</td><td>300</td><td>2432</td></tr><tr><td>Nvidia A100 (reference)</td><td>~$12,000</td><td>-</td><td>1000</td></tr></tbody></table><p>Typically, larger models need larger batch sizes for speed and generalization. However, GPUs have limited memory and cannot scale to the needs of a billion-parameter network. We found that a top-of-the-line A100 GPU with 48GB of memory can barely accommodate a batch size of 256 for our 1.6B network.</p><p>On this giant model, a batch size of 2048&ndash;which gives better accuracy than a batch size of 256&ndash;runs out of memory on an A100 GPU. Even distributing the model on two A100s is not enough; we need at least four A100 GPUs and nearly 200GB of memory to train the 1.6 billion parameter model with a batch size of 2048.</p><h2 id=training-a-16b-parameter-model>Training a 1.6B Parameter Model</h2><img class=gpu-comparison alt="Training Time vs. CPU"><p>On the other hand, our BOLT engine can effortlessly scale to a large batch size with no change in model memory. We can even train with a batch size of 10,000. Notably, training this model using TensorFlow on CPUs is around 5x slower than BOLT.</p><table><thead><tr><th>Processor</th><th>Average price (USD)</th><th>Training time per epoch: BOLT (s)</th><th>Training time per epoch: TensorFlow (s)</th></tr></thead><tbody><tr><td>Intel Xeon E5-2680</td><td>~$1,200</td><td>1369</td><td>5875</td></tr><tr><td>Nvidia A100</td><td>~$12,000</td><td>-</td><td>Insufficient memory</td></tr><tr><td>2x Nvidia A100</td><td>~$24,000</td><td>-</td><td>Insufficient memory</td></tr><tr><td>4x Nvidia A100</td><td>~$50,000</td><td>-</td><td>1648</td></tr></tbody></table><p>A billion parameter model with a batch size of 2048 requires a fleet of four A100s to train, each costing more than $10K. In contrast, our BOLT engine can train the same model faster on a 24-core refurbished V3 Intel machine that costs less than $1,200.</p><p>Eager to try it out yourself? <a href=//thirdailabs.github.io/contact/ title=thirdai.com/contact>Contact us</a> for more details or <a href=//thirdailabs.github.io/resources/demo>request a demo</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=//thirdailabs.github.io/tags/case-study/>Case Study</a></li><li><a href=//thirdailabs.github.io/tags/product-recommendation/>Product Recommendation</a></li><li><a href=//thirdailabs.github.io/tags/bolt/>BOLT</a></li></ul><nav class=paginav><a class=next href=//thirdailabs.github.io/solutions/question-answering/><span class=title>»</span><br><span>Question Answering and Information Retrieval</span></a></nav></footer></article></main><footer class=fat-footer><div class=wrapper><div class=fat-footer-social><a href=https://twitter.com/ThirdAILab><img src=//thirdailabs.github.io/images/twitter.svg width=50px class=footer-icon></a>
<a href=https://www.linkedin.com/company/thirdai-corp><img src=//thirdailabs.github.io/images/linkedin.svg width=45px class=footer-icon></a>
<a href=#><img src=//thirdailabs.github.io/images/facebook.svg width=22px class=footer-icon></a></div><div class=fat-footer-menus><div class=menu-container><div class=menu-title><h3>solutions</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a></li><li><a href=//thirdailabs.github.io/solutions/text-classification>Text Classification</a></li><li><a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>engine</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/engine/bolt>BOLT</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>resources</h3></div><div class=menu-content><ul><li><a href=#>Documentation</a></li><li><a href=#>Webinars</a></li><li><a href=//thirdailabs.github.io/resources/blog>Blog</a></li><li><a href=//thirdailabs.github.io/resources/demo>Demo</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>company</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/company/about>About us</a></li><li><a href=//thirdailabs.github.io/company/careers>Careers</a></li><li><a href=//thirdailabs.github.io/contact>Contact</a></li></ul></div></div></div></div><div class=copyright-wrapper><div class=logo><a href=//thirdailabs.github.io/><img src=//thirdailabs.github.io/images/logo-dark.png alt=logo height=35></img>ThirdAI</a></div><div class=copyright><span>&copy; 2022 <a href=//thirdailabs.github.io/>Third AI</a></span></div></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>