<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Question Answering and Information Retrieval | ThirdAI</title><meta name=keywords content="Case Study,Question Answering,Information Retrieval,BOLT"><meta name=description content="Our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches for question answering."><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=//thirdailabs.github.io/static/css/stylesheet.min.e201a79b160519992064e8c74bca3b41ed8345ed8333de3af8af3a3c69209949.css integrity="sha256-4gGnmxYFGZkgZOjHS8o7Qe2DRe2DM946+K86PGkgmUk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=//thirdailabs.github.io/static/js/highlight.min.b801c3817c6b951b39b4f5adde05c84b6d72be498309c6b09454d361bf3f3b02.js integrity="sha256-uAHDgXxrlRs5tPWt3gXIS21yvkmDCcawlFTTYb8/OwI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.100.2"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Question Answering and Information Retrieval"><meta property="og:description" content="Our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches for question answering."><meta property="og:type" content="article"><meta property="og:url" content="//thirdailabs.github.io/solutions/question-answering/"><meta property="og:image" content="//thirdailabs.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="solutions"><meta property="og:site_name" content="ThirdAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="//thirdailabs.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Question Answering and Information Retrieval"><meta name=twitter:description content="Our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches for question answering."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Solutions","item":"//thirdailabs.github.io/solutions/"},{"@type":"ListItem","position":2,"name":"Question Answering and Information Retrieval","item":"//thirdailabs.github.io/solutions/question-answering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Question Answering and Information Retrieval","name":"Question Answering and Information Retrieval","description":"Our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches for question answering.","keywords":["Case Study","Question Answering","Information Retrieval","BOLT"],"articleBody":" A single fundamental technology lies at the heart of many modern services, including ed-tech, enterprise search, and automated customer service: question answering. Given a large corpus of documents, a good question answering system responds to a question by returning a phrase or sentence from one of the documents that answers the question.\nModern question answering systems have two pieces: a document search component that narrows the search space by finding a top few promising documents, and a machine learning highlighting model that highlights the best passage within the returned documents.\nFor example, given an input question string like “when did Beyoncé start becoming popular?”, such a system might return the following paragraph from Wikipedia:\n“Beyoncé Giselle Knowles-Carter (bee-YON-say; born September 4, 1981) is an American singer, songwriter, and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child. She rose to fame in the late 1990s as the lead singer of Destiny’s Child, one of the best-selling girl groups of all time. Their hiatus saw the release of her debut album Dangerously in Love (2003), which featured the US Billboard Hot 100 number-one singles “Crazy in Love” and “Baby Boy.”\nOnce the system finds this relevant paragraph, the highlighting model might highlight and return the phrase:\n“She rose to fame in the late 1990s as the lead singer of Destiny’s Child.”\nNowadays such highlighting models are standard in packages like huggingface. The harder part is quickly and accurately finding relevant paragraphs out of tens of millions of possibilities, which is what we focus on here.\nHow about current solutions? Historically, one might use lexical search based solutions like Elastic Search to filter for matching documents. However, the limitations of text matching and lexical search techniques are clear on search benchmarks like MS MARCO, where lexical methods have half the accuracy compared to the state-of-the-art methods. As a concrete example, in the Beyonce excerpt above, the only keyword that the query and paragraph have in common is “Beyoncé”. Keyword matching systems may not be able to determine that “fame” and “popular” are related.\nThis is where cutting-edge deep learning models step in. Models like ColBERT achieve significantly higher accuracy on those same benchmarks. They operate in three fundamental steps:\nObtain a pre-trained language model and fine-tune it on your data Encode your documents using the learned model and build a searchable index out of the encodings Encode user queries using the same model, and then search against all document encodings However, deep learning models also require expensive GPU machines and several days to train and index. Furthermore, in production use cases, new documents may need to be added to the index every day or hour, and learned models will drift from reality. This necessitates constant re-training and re-indexing, which is simply too expensive and thus infeasible for most companies.\nThe woes do not stop here: question answering is also latency-critical. Any system expecting users to wait more than 100ms per query will likely see poor adoption. Systems like ColBERT achieve state of the art accuracy, but do not reach the lightning-fast latencies of lexical search.\nThe ThirdAI Difference We’ve built an engine that allows us to perform training and inference of large models on commodity CPUs extremely quickly.\nOne of the modules we’ve added to BOLT allows us to achieve highly-efficient document search; we’ve summarized our results on the MS MARCO benchmark in the table below:\nMethod, all with 16 vCPUs, k=1000 P50 (ms/query) P95 (ms/query) P99 (ms/query) MRR@10 R1@50 R1@1000 ColBERT v2 (SOTA), CPU 721 873 949 0.395 0.856 0.965 ThirdAI’s BOLT (Large) 100 129 140 0.386 0.850 0.962 ThirdAI’s BOLT (Small) 71 93 101 0.355 0.821 0.954 Elastic Search 53 98 136 0.174 0.550 0.809 P50 is the end-to-end query latency at the 50th percentile, meaning 50% of queries will see latency less than that number (P95 and P99 are identical except with 95th and 99th percentiles, respectively). MRR is the mean reciprocal rank of the best document within the first 10 returned results. R1@k is the percent recall of the best document within the top k retrieved documents. We ran this experiment on the curated small MS MARCO dev step which is standard in research papers.\nThus, our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches.\nEager to try it out yourself? Contact us for more details or request a demo.\n","wordCount":"742","inLanguage":"en","image":"//thirdailabs.github.io/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"//thirdailabs.github.io/solutions/question-answering/"},"publisher":{"@type":"Organization","name":"ThirdAI","logo":{"@type":"ImageObject","url":"//thirdailabs.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=list id=top><script>window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><div class=hero><header class=header><nav class=nav><div class=logo><a href=//thirdailabs.github.io/ accesskey=h title="ThirdAI (Alt + H)"><img class=icon alt=logo aria-label=logo height=35></img>ThirdAI</a>
<span class=logo-switches></span></div><ul id=menu><li><div class=dropdown><button class=drop-button>Solutions</button><div class=drop-content><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a>
<a href=//thirdailabs.github.io/solutions/text-classification>Sentiment Classification</a>
<a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></div></div></li><li><a href=//thirdailabs.github.io/engine/bolt><div class=menu-item>Engine</div></a></li><li><div class=dropdown><button class=drop-button>Resources</button><div class=drop-content><a href=//thirdailabs.github.io/resources/documentation>Documentation</a>
<a href=//thirdailabs.github.io/resources/webinars>Webinars</a>
<a href=//thirdailabs.github.io/resources/blog>Blog</a>
<a href=//thirdailabs.github.io/resources/demo>Demo</a></div></div></li><li><div class=dropdown><button class=drop-button>Company</button><div class=drop-content><a href=//thirdailabs.github.io/company/about>About</a>
<a href=//thirdailabs.github.io/company/careers/>Careers</a></div></div></li><li><a href=//thirdailabs.github.io/contact/><div class=menu-item>Contact</div></a></li></ul><input class=checkbox type=checkbox><div class=hamburger-lines><span class="line line1"></span>
<span class="line line2"></span>
<span class="line line3"></span></div><div class=mobile-menu-items><li><a href=//thirdailabs.github.io/solutions/>Solutions</a></li><li><a href=//thirdailabs.github.io/engine/bolt>Engine</a></li><li><a href=//thirdailabs.github.io/resources/>Resources</a></li><li><a href=//thirdailabs.github.io/company/>Company</a></li><li><a href=//thirdailabs.github.io/contact/>Contact</a></li></div></nav></header></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//thirdailabs.github.io/>Home</a>&nbsp;»&nbsp;<a href=//thirdailabs.github.io/solutions/>Solutions</a>&nbsp;»&nbsp;<a>Question Answering and Information Retrieval</a></div><h1 class=post-title>A Document Search Case Study: Sub 100ms Latency with 8 Million Documents</h1><div class=post-meta></div></header><div class=post-content><img class=doc-search-graphic alt="Document Search Graphic"><p>A single fundamental technology lies at the heart of many modern services, including ed-tech, enterprise search, and automated customer service: question answering. Given a large corpus of documents, a good question answering system responds to a question by returning a phrase or sentence from one of the documents that answers the question.</p><p>Modern question answering systems have two pieces: a document search component that narrows the search space by finding a top few promising documents, and a machine learning highlighting model that highlights the best passage within the returned documents.</p><p>For example, given an input question string like “when did Beyoncé start becoming popular?”, such a system might return the following paragraph from Wikipedia:</p><blockquote><p>“Beyoncé Giselle Knowles-Carter (bee-YON-say; born September 4, 1981) is an American singer, songwriter, and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child. She rose to fame in the late 1990s as the lead singer of Destiny’s Child, one of the best-selling girl groups of all time. Their hiatus saw the release of her debut album Dangerously in Love (2003), which featured the US Billboard Hot 100 number-one singles “Crazy in Love” and “Baby Boy.”</p></blockquote><p>Once the system finds this relevant paragraph, the highlighting model might highlight and return the phrase:</p><blockquote><p>&ldquo;She rose to fame in the late 1990s as the lead singer of Destiny’s Child.&rdquo;</p></blockquote><p>Nowadays such highlighting models are standard in packages like <a href=https://huggingface.co/>huggingface</a>. The harder part is quickly and accurately finding relevant paragraphs out of tens of millions of possibilities, which is what we focus on here.</p><h2 id=how-about-current-solutions>How about current solutions?</h2><img class=latency-graph alt="Latency vs. Model Graph"><p>Historically, one might use lexical search based solutions like Elastic Search to filter for matching documents. However, the limitations of text matching and lexical search techniques are clear on search benchmarks like <a href=https://microsoft.github.io/msmarco/>MS MARCO</a>, where lexical methods have half the accuracy compared to the state-of-the-art methods. As a concrete example, in the Beyonce excerpt above, the only keyword that the query and paragraph have in common is “Beyoncé”. Keyword matching systems may not be able to determine that “fame” and “popular” are related.</p><p>This is where cutting-edge deep learning models step in. Models like <a href=https://github.com/stanford-futuredata/ColBERT>ColBERT</a> achieve significantly higher accuracy on those same benchmarks. They operate in three fundamental steps:</p><ol><li>Obtain a pre-trained language model and fine-tune it on your data</li><li>Encode your documents using the learned model and build a searchable index out of the encodings</li><li>Encode user queries using the same model, and then search against all document encodings</li></ol><p>However, deep learning models also require expensive GPU machines and several days to train and index. Furthermore, in production use cases, new documents may need to be added to the index every day or hour, and learned models will drift from reality. This necessitates constant re-training and re-indexing, which is simply too expensive and thus infeasible for most companies.</p><p>The woes do not stop here: question answering is also latency-critical. Any system expecting users to wait more than 100ms per query will likely see poor adoption. Systems like ColBERT achieve state of the art accuracy, but do not reach the lightning-fast latencies of lexical search.</p><h2 id=the-thirdai-difference>The ThirdAI Difference</h2><p>We&rsquo;ve built an <a href=https://www.thirdai.com/bolt>engine</a> that allows us to perform training and inference of large models on commodity CPUs extremely quickly.</p><img class=recall-graph alt="Recall vs. Model Graph"><p>One of the modules we&rsquo;ve added to BOLT allows us to achieve highly-efficient document search; we&rsquo;ve summarized our results on the <a href=https://microsoft.github.io/msmarco/>MS MARCO</a> benchmark in the table below:</p><table><thead><tr><th>Method, all with 16 vCPUs, k=1000</th><th>P50 (ms/query)</th><th>P95 (ms/query)</th><th>P99 (ms/query)</th><th>MRR@10</th><th>R1@50</th><th>R1@1000</th></tr></thead><tbody><tr><td>ColBERT v2 (SOTA), CPU</td><td>721</td><td>873</td><td>949</td><td>0.395</td><td>0.856</td><td>0.965</td></tr><tr><td>ThirdAI&rsquo;s BOLT (Large)</td><td>100</td><td>129</td><td>140</td><td>0.386</td><td>0.850</td><td>0.962</td></tr><tr><td>ThirdAI&rsquo;s BOLT (Small)</td><td>71</td><td>93</td><td>101</td><td>0.355</td><td>0.821</td><td>0.954</td></tr><tr><td>Elastic Search</td><td>53</td><td>98</td><td>136</td><td>0.174</td><td>0.550</td><td>0.809</td></tr></tbody></table><p>P50 is the end-to-end query latency at the 50th percentile, meaning 50% of queries will see latency less than that number (P95 and P99 are identical except with 95th and 99th percentiles, respectively). MRR is the <a href=https://en.wikipedia.org/wiki/Mean_reciprocal_rank>mean reciprocal rank</a> of the best document within the first 10 returned results. R1@<em>k</em> is the percent recall of the best document within the top <em>k</em> retrieved documents. We ran this experiment on the curated small MS MARCO dev step which is standard in research papers.</p><p>Thus, our BOLT engine obtains the speed and scalability of lexical search with the accuracy of the best deep learning approaches.</p><p>Eager to try it out yourself? <a href=//thirdailabs.github.io/contact/ title=thirdai.com/contact>Contact us</a> for more details or <a href=//thirdailabs.github.io/resources/demo>request a demo</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=//thirdailabs.github.io/tags/case-study/>Case Study</a></li><li><a href=//thirdailabs.github.io/tags/question-answering/>Question Answering</a></li><li><a href=//thirdailabs.github.io/tags/information-retrieval/>Information Retrieval</a></li><li><a href=//thirdailabs.github.io/tags/bolt/>BOLT</a></li></ul><nav class=paginav><a class=prev href=//thirdailabs.github.io/solutions/product-recommendation/><span class=title>«</span><br><span>Product Recommendation</span></a>
<a class=next href=//thirdailabs.github.io/solutions/text-classification/><span class=title>»</span><br><span>Sentiment Classification</span></a></nav></footer></article></main><footer class=fat-footer><div class=wrapper><div class=fat-footer-social><a href=https://twitter.com/ThirdAILab><img src=//thirdailabs.github.io/images/twitter.svg width=50px class=footer-icon></a>
<a href=https://www.linkedin.com/company/thirdai-corp><img src=//thirdailabs.github.io/images/linkedin.svg width=45px class=footer-icon></a>
<a href=#><img src=//thirdailabs.github.io/images/facebook.svg width=22px class=footer-icon></a></div><div class=fat-footer-menus><div class=menu-container><div class=menu-title><h3>solutions</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/solutions/question-answering>Question Answering</a></li><li><a href=//thirdailabs.github.io/solutions/text-classification>Text Classification</a></li><li><a href=//thirdailabs.github.io/solutions/product-recommendation>Product Recommendation</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>engine</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/engine/bolt>BOLT</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>resources</h3></div><div class=menu-content><ul><li><a href=#>Documentation</a></li><li><a href=#>Webinars</a></li><li><a href=//thirdailabs.github.io/resources/blog>Blog</a></li><li><a href=//thirdailabs.github.io/resources/demo>Demo</a></li></ul></div></div><div class=menu-container><div class=menu-title><h3>company</h3></div><div class=menu-content><ul><li><a href=//thirdailabs.github.io/company/about>About us</a></li><li><a href=//thirdailabs.github.io/company/careers>Careers</a></li><li><a href=//thirdailabs.github.io/contact>Contact</a></li></ul></div></div></div></div><div class=copyright-wrapper><div class=logo><a href=//thirdailabs.github.io/><img src=//thirdailabs.github.io/images/logo-dark.png alt=logo height=35></img>ThirdAI</a></div><div class=copyright><span>&copy; 2022 <a href=//thirdailabs.github.io/>ThirdAI</a></span></div></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll('pre > code').forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement('button');e.classList.add('copy-code'),e.innerText='copy';function s(){e.innerText='copied!',setTimeout(()=>{e.innerText='copy'},2e3)}e.addEventListener('click',o=>{if('clipboard'in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand('copy'),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>